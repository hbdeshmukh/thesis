\chapter{Conclusions and Future Work}
In this chapter, we discuss the lessons learnt, present our conclusions, and describe the future work.
Through this dissertation, we present the design and implementation of a query scheduler.
The goal of this scheduler is to efficiently execute queries in an in-memory environment while effectively exploiting the features provided by the modern hardware. 
A key contribution of this dissertation is to demonstrate that the scheduler is capable of playing an important role in the database system architecture.

\section{Lessons Learnt}
We now discuss the lessons learnt from this dissertation.

\subsection{Importance of Abstraction and Contracts}
A good abstraction is vital for developing a good system. 
The work order abstraction in Quickstep proved to be a boon for the query scheduler. 
It allowed us to construct different layers on top of simple operator execution, for which the  abstraction was created. 

There was a clarity of the contract between query optimizer and query scheduler: the query optimizer creates an immutable query plan (DAG of relational operators), which is then passed on to the query scheduler. 
Once the query scheduler starts to operate on the query plan, there is no going back to the optimizer. 

We designed the operators to be stateful and allowed the scheduler to access their state thereby getting more insights about the query execution progress.
The book-keeping related data structures are handled only by the scheduler thread, thus they need not be thread-safe, which helped simplify their design. 

\subsection{Importance of Query Scheduler for High Performance}
Achieving high performance is a never unending pursuit for database systems. 
Though the scheduler has a lot of power in terms of controlling resources and co-ordinating the query execution, it still needs support from other modules in the database system for high performance.
Query optimizer is a vital component of the system. 
Scheduling work for an optimal query plan is far more effective than that for a poor plan. 
Moreover, efficient implementation of relational operators is crucial to leverage the most from the hardware. 
Therefore, even though the scheduler is an important piece in the overall puzzle, it must work in tandem with other components in the system. 

\subsection{Importance of Measurement, Estimation, and Analysis}
Measuring and estimating performance in systems is highly important. 
Back-of-the-envelope calculations is a handy technique that can be useful to anticipate the effect of a technique or a change.
By doing such calculations, we may be able to save a lot of development effort. 

Our work on revisiting pipelining relied heavily on measuring performance and analyzing the impact of various techniques on performance. 
Putting the performance improvement of pipelining into perspective was only possible due to the emphasis on measurement.
We made lot of mistakes in attributing performance changes to wrong reasons and learnt some valuable lessons the hard way.

While dealing with many parameters, it is important to adhere to basic scientific principles, e.g, not changing two parameters simultaneously, while studying the effect of one.

%Between multi-core CPU and memory, which are the primary kinds of resources used by an in-memory database system; CPU is 
\section{Conclusions}
We now discuss the conclusions from individual chapters of this dissertation. 

In Chapter~\ref{chap:quickstep}, we present the design and implementation of the Quickstep database system. 
Quickstep has a unique block based storage management system, that supports variety of storage formats such as row store, column store along with support for compression. 
The query execution in Quickstep happens with the help of a task abstraction called work orders. 
The work orders abstraction connects storage management in Quickstep with the execution engine. 
Quickstep's query scheduler, which is the focus of this dissertation controls the execution of these work orders and thus manages query execution in the system. 
Through our experimental evaluation, we show that the intra-operator and inter-operator parallelism through the work orders abstraction speeds up the query execution in Quickstep.
The work order abstraction is instrumental for realizing intra-operator parallellism.
Comparison with other systems also show that Quickstep is faster than other systems, often by orders of magnitude. 

In Chapters~\ref{chap:policy} and Chapter~\ref{chap:pipeline}, we show that the scheduler can be used to meet multiple objectives.
In Chapter~\ref{chap:policy} we use the scheduler for the problem of resource governance.
We specifically target the problem of allocating resources such as CPU to concurrent queries according to well defined policies.
A key challenge in this problem is the fluctuation in the resource requirements of a query.
We target real time queries setting, in which queries could arrive at any time, thus it further complicates the problem.
Our solution uses a probabilistic framework for taking the scheduling decisions and thereby allocating resources to queries.
The probabilistic framework is backed by a learning agent, that monitors the resource consumption in the system and using learning techniques, predicts the resource requirements of queries in the near future.  
We form a close loop between scheduling decisions and the learning, where the learning agent monitors the system, predicts the future resource requirements and tweaks the probabilities. 
The adjusted probabilities result in change in the resource allocation, leading the system closer to meeting the high level policy goals.
Our experimental evaluation shows that Quickstep scheduler can meet the goals for a variety of policies including fair and priority-based. 

Chapter~\ref{chap:pipeline} uses the scheduler for the performance objective.
Here we analyze the role of pipelining in the in-memory settings. 
We cast pipelining as a schedule in which consumer tasks (or work orders) are interleaved with the producer tasks. 
We compare pipelining with a non-pipelining strategy in which the said interleaving is not present. 
The two strategies differ primarily in the eagerness of data processing by the consumer operator. 
We underscore the impact of various parameters such as number of threads, the size of blocks, and hardware prefetching on the relative performance of the two pipelining strategies. 
Our analysis (based on evaluation on TPC-H queries as well as an analytical model and microbenchmarking) shows that the relative gap between the two strategies is not as high as in the disk based settings. 
We highlight that the impact of pipelining on query processing should be considered holistically with all the parameters listed earlier. 
%One key contribution through our study is the importance of degree of parallelism in query performance. 
%Pipelining can provide superior performance in cases when operators in the pipeline have scalability issues (with increasing number of threads). 
%We present an algorithm that sequences the various pipelines in a query plan with the goal of maximizing query performance.  
%Our evaluation on TPC-H queries show that the algorithm often produces pipeline sequences with better performance than other pipeline sequences. 

\subsection{Future Work}
Our scheduling work leads to many interesting future work directions.
There is an ongoing effort to make Quickstep work efficiently on a distributed setup. 
The distributed setting offers some challenges which are not as relevant in the in-memory single node setting.
Locality of a work order execution is quite important in the distributed setting.
The penalty of a non-local execution involves high data movement cost over network, which can degrade query performance.

Load balancing is another important aspect that a distributed query scheduler has to worry about.
Overloading a node with work can throttle the query processing and may cause bottlenecks.
A subtle aspect in distributed query scheduling is that the control traffic (e.g. dispatching a work order, reeving work order completion feedback) may be expensive.
Therefore the scheduler may have to batch or aggregate control messages.

In the single node setting, the scheduler can play a big role in query processing on heterogeneous hardware.
An example of such system can be a server configured with an FPGA or a GPU.
If the scheduler can understand the strengths and weaknesses of each heterogeneous hardware component, it can route work orders to different components appropriately. 
A key challenge in this problem is to understand the memory access costs for different hardware components and thus estimating the cost of executing operations.