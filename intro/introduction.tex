\chapter{Introduction}
%\paragraph{Scheduling on a high level}
Computer systems need resources to function. 
Usually the availability of such resources is limited and the system has to share these resources among multiple users. 
Scheduling can be viewed as a governance model that \textit{governs} how the resources are shared among users and how various parts of the system function together. 
The governance model is designed so as to meet goals about resource management and performance of the system. 
As an example, an operating system uses resources such as memory, CPUs, and disk.
It allows multiple processes to use these resources at the same time. 

%\paragraph{Scheduling's history}
Scheduling has been studied extensively in many communities including computer science theory, computer systems, and industrial systems. 
Typically a scheduling problems involves finding a schedule with a certain objective.
The desired solution is the one that either meets the objective or gets close to it.
Many scheduling problems are difficult in nature.
For example, the problem of scheduling $N$ jobs each with a unit execution time on $k$ processors such that the makespan of the schedule is minimized is NP complete~\cite{ULLMAN1975384}.
Some of the standard techniques used to solve scheduling problems are: using domain specific heuristics, dynamic programming, greedy algorithms, and approximation algorithms. 

%\paragraph{Scheduling for databases}
In this dissertation, we study the problem of scheduling for modern database systems.
Before understanding the scheduling problem itself, we first understand what makes the modern database systems \textit{modern}.

\section{Modern Database Systems and the need for Scheduling}
We distinguish traditional database systems and modern database systems based on three aspects which are closely related to the scheduling problem: modern hardware, growing data volumes, and deployment environments.
We discuss these aspects below and explain how they are related to the scheduling problem. 
\paragraph{Modern hardware:} 
Over the past few years hardware has drastically evolved. 
Modern hardware includes large main memories, Non-Uniform Memory Access (NUMA) patterns and large number of CPU cores.
%To leverage the capabilities of the modern hardware comprehensively, database architecture needs a fundamental shift from its traditional roots. 
Traditional database architecture comes from a time when memories were smaller, data were mostly resident on disks and multi-core parallelism was uncommon. 
Therefore modern hardware have high availability of two resources as compared to their traditional counterparts: CPU parallelism and memory.
Scheduling involves managing resources, therefore we believe that there is a place for schedulers in modern database architecture. 
\paragraph{Growing data volumes:}
We are in the era of big data and witnessing tremendous amount of data being generated.
To process the large amount of data efficiently, data processing engines have to effectively leverage all the hardware features. 
However, what we are experiencing is a growing \textit{deficit} between the pace of hardware performance improvements and the pace that is demanded of data processing kernels to keep up with the growth in data volumes.

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{system/figures/deficit.pdf}
%	\vspace*{-2em}
	\caption{\textbf{Processor performance improvement as measured by the highest reported CINT2006 benchmark result for Intel Xeon chips from~\cite{cpu2006} compared to the number of pages indexed by Google (using estimates made by~\cite{google-pages-db}). The figure does not show the increase in the number of queries (which is about 2.5X for Google search queries from 2011--14), and the increase in the complexity of queries as applications request richer analytics. These aspects make the deficit problem worse. The figure also shows the maximum number of cores per chip used in reported CINT2006 results over time. Interestingly (and not shown in the figure), both the minimum and the average amount of memory per chip in the reported CINT2006 results has grown by $\approx$4X from 2011 to 2017.}}
	% For example, the number of Google search queries has gone up by a factor of 6X in the time period shown in the graph above.}
	\label{fig-deficit}
\end{figure}

Figure~\ref{fig-deficit} illustrates this deficit issue by comparing improvements in processor performance (blue line) with the growth rate of data (green line), using the number of pages indexed by Google as an illustrative example. 
This data growth rate is conservative for many organizations, which tend to see a far higher rate of increase in the data volume; for example, Facebook's warehouse grew by 3X in 2014~\cite{fb-growth-14}. 
This figure also shows (using a dotted orange line with squares) the growth in the number of cores per processor over time. 
As one can observe, the number of cores per processor is rising rapidly. % as multi-core parallelism is critical for processor vendors.
%to realizing overall higher processor performance.
%(While we do not consider this further in this paper, the number of cores per processing unit is even higher for non-traditional processors/co-processors.)
% Thus, the demands on query processing can't simply be met by only throwing more chips/processors at the problem.
%Thus, there is a critical need for systems that can exploit the full potential of the hardware parallelism that is available in each box.
In addition, %as noted in the caption of the figure,
since 2011 the main memory sizes are also growing rapidly, and there is an increasing shift to larger main memory configurations. 

In addition to the data growth, there is a growing interest in finding more insights from the data as quickly as possible.
In addition to the traditional online analytical processing (OLAP), there is an increased interest in advanced analytics~\cite{DBLP:conf/sigmod/Kumar0017} which involves machine learning algorithms, both from academic community as well as industry. 
Thus, there is a critical need for in-memory data processing methods that \textit{scale-up} to exploit the full (parallel) processing power that is locked in commodity multi-core servers today.

\paragraph{Deployment environments:} 
The most common deployment method few years ago was "on-premise database", which means an enterprise would host and maintain the database software on local servers. 
All the deployment related concerns such as failover handling, installing additional capacity, upgrading hardware would be taken care by the in-house database administrator. 
Today cloud databases have drastically changed how databases are deployed.
Cloud providers host and manage databases on their cloud platforms and abstract away all the deployment pain points from the users.
The database engine may be hosted on a number of hardware platforms or virtualized environments in the cloud.

Cloud database is still expected to offer the best performance and meet the Service Layer Objectives (SLO) promised to the client of the database service. 
To meet the SLO, the database service may suddenly need to elastically add more nodes. Sometimes to reduce operational expenses and to lower the energy consumption, the database service may need to downsize the number of operational nodes.

Some cloud databases provide differentiated service guarantees to users based on their importance levels. 
To guarantee the SLO for all users, the database service may require to reshuffle resources from less important customers to more important customers. 

In summary, the resource management layer of the cloud database needs to be flexible enough to withstand dynamicity in the resource availability.
Hence the scheduler should have built in resource allocation flexibility, helping the resource management of the database become dynamic.