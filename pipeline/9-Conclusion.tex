\section{Conclusion and Future Work}\label{sec:conclusions}
We revisited pipelining in the in-memory environments for database systems which use a block-based query processing with high intra-operator parallelism.
We discussed many dimensions that impact the performance of pipelining and non-pipelining strategies of query processing and performed extensive experiments to showcase the performance of each strategy.
We also present an analytical model that estimates the performance of each strategy as a function of block size and cache misses. 
Our analytical model, as well as empirical evaluation on \sys{} database system shows that pipelining and non-pipelining strategies are not very different w.r.t query performance. 
We discuss reasons for their similarities and dissimilarities throughout the paper. 
One key conslusion is that the impact of pipelining on a query plan largely depends on the structure of the query and the time distribution across its operators.
Though pipelining speeds up some consumer operators due to better cache behavior, the improvements are not substantial when considered w.r.t to the total query execution time.
We showcase a surprising effect of scalability of operators on the performance of pipelining and non-pipelining strategies.

For the future work, \sys{} continues to evolve.
We aim to build a distributed version of the system, in which the pipelining problem will take a different form.
It will be interesting to study the pipelining problem for in-memory clusters with fast networks, which drastically reduce the data transmission time from one node to another. 
One fascinating direction of future study is pipelining in the heterogenous hardware environments, where data movement cost across two compute units (e.g. CPU to GPU vs CPU to CPU) could be different. 
