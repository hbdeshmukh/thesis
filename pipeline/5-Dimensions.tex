\section{Discussion on Dimensions}\label{sec:dimensions}
The performance of pipelining and non-pipelining strategies depends upon multiple factors associated with query processing.
We broadly classify these dimensions in three categories: physical organization of data (storage format and block size), execution environment (parallelism and hardware characteristics), and structural aspects of query (pipeline sequence).
In this section we discuss these dimensions.

\subsection{Storage Format}
Data processing time is impacted by the way data are organized.
We look at two common storage formats: row store and column store.
In column store format, values of a given column are stored in contiguous memory region.
Accessing a single column which has a sequential access pattern, results in a good cache behavior.
In row store format, all columns of a tuple are stored in a contiguous region. 
Thus accessing a particular column involves bringing unnecessary data (non-referenced columns) in the caches. 

Column stores have shown to have higher performance for analytical workloads~\cite{DBLP:conf/sigmod/AbadiMH08}. 
The difference between the performance of column store and row store format is prominent for scan operators.
Typically scans are faster with column store than in row store.

%To explore the relationship between pipelining performance and storage formats, we use two storage formats for our evaluation namely row store and column store.
Recent studies~\cite{quickstep-system} have shown that the performance gap between column stores and row stores is not as high as shown in the prior work.
Therefore we experiment with both storage formats. 
For our comparison, all the base tables in the TPC-H schema are stored in the same storage format.
We use row store format for temporary tables irrespective of the storage format of the base tables.

\subsection{Block Size}
We start by explaning the concept of block size.
As the producer operator processes the input, it materializes the output to a temporary block. 
We let the temporary block to reach a threshold size (known as block size) and then pass the block to the consumer operator. 

We would like to examine the impact of block size on the performance of the pipelining strategies. 
Consider an example pipeline: select operator $\rightarrow$ probe hash table operator.
A smaller block size in the pipelining strategy would mean that the block can potentially get filled quickly. 
Therefore, compared to a larger block size, the probe tasks may be scheduled more frequently and each task itself may be shorter. 

\subsection{Parallelism}
Intra-operator parallelism is prevalent in many modern database systems~\cite{monetdb, morsel, quickstep-system, vectorwise}.
Pipelining encapsulates inter-operator parallelism, as it exemplifies simultaneous execution of producer and consumer operators in a pipeline.
We would like to study the impact of parallelism on the relative performance of pipelining and non-pipelining.

We define some terminologies that are relevant for our study in the context of concurrency.
\textit{Degree of parallelism} (DOP) of an operator refers to the number of concurrent threads involved in executing \wo{}s of that operator. 
The \textit{scalability} of an operator (using $T$ threads) is its performance with DOP as $T$ relative to its performance when DOP is 1.

We now discuss the DOP behavior of operators in \sys{}.
Recall from Section~\ref{ssec:system-scheduling} that \sys{} has a scheduler that dispatches \wo{}s  of relational operators to worker threads.
In \sys{}, the pipelining strategy can result in a fluid DOP of operators.
Consider the example from Figure~\ref{fig:pipelining-schedules}.
Until there is enough input available to schedule a probe \wo{}, all worker threads operate on filter operator, thus the DOP of the filter operator is same as number of threads in the system.
As soon as a probe \wo{} is generated it gets scheduled, at which time the DOP of the probe operator is 1.
If multiple probe \wo{} are generated simultaneously, the DOP of the probe operator could be higher.

In the non-pipelining strategy, the DOP of operators remains roughly the same.
As shown in Figure~\ref{fig:pipelining-schedules} for the non-pipelining schedule, the DOP of select and probe operator remain maximum (equal to number of threads).  

\subsubsection{Scalability}\label{sssec:scalability}
In an ideal environment, adding more threads for an operator execution should offer linear speedup.
The assumption being that each parallel \wo{} operates at the same speed and thus by executing more tasks concurrently, the overall execution time reduces proportionally. 

Linear speedups for operators (or for queries as a whole) are not always possible.
DeWitt and Gray~\cite{DBLP:journals/cacm/DeWittG92} propose reasons for less than ideal speedup for parallel databases such as startup costs, interference from concurrent execution and skew.
We can extend some of their ideas to in-memory systems.
For example interference can come from various sources such as contention due to latches, and shared use of a common bandwidth like memory bandwidth or bandwidth for data movement across NUMA sockets.

For an operator that exhibits poor scalability, increasing its DOP beyond a limit degrades its performance.
Specifically the execution time for each \wo{} of the operator increases. 
We observe that poor speedup can have contrasting impact on pipelining (positive) and non-pipelining (negative) strategies.
The reason for such contrasting behavior lies in the difference in the DOP values of a consumer operator in query processing with and without pipelining.
Recall from Figure~\ref{fig:pipelining-schedules} that pipelining yields in a lower DOP for the consumer operator, where as non-pipelining results in a higher DOP for the consumer operator.

\subsection{Hardware Prefetching}
We start by explaining what hardware prefetching is. 
It is a technique used by modern hardware to proactively fetch data in caches by speculating its access in the future.
The prefetcher observes patterns of data accesses from memory to caches and speculates the access of a data element in advance.
Prefetching hides the latency due to a cache miss and thus potentially improves performance.
There are two kinds of prefetching: spatial and temporal, among which we focus on spatial prefetching. 

Now we describe why prefetching is important to our study. 
Pipelining involves a context switch on the kinds of \wo{}s that get executed (c.f. Figure~\ref{fig:pipelining-schedules}).
Thus pipelining may affect prefetcher's understanding of data access pattern. 
Therefore we are specifically interested in the impact of hardware prefetching on pipelined query processing. 
%Spatial prefetching is relevant when there is a sequential access pattern. 
%For example, if a data element at position $i$ is accessed at time $t$, it is likely that at time $t + 1$, the data element at position $i + 1$ will be accessed.
%Thus the data element at position $i + 1$ can be prefetched to improve performance.
%Temporal prefetching is associated with repeated accesses of the same data element.

In addition to the hardware-based prefetching implementation, there are software-based techniques for prefetching.
There is prior work on using software-based prefetching to improve the performance of database operators~\cite{DBLP:conf/icde/ChenAGM04, rof}.
We restrict ourselves to hardware-based prefetching, as we can observe the impact of hardware prefetcher without modifying the implementation of the relational operators.

For our study, we run the queries with pipelining in two scenarios: a) hardware prefetching is enabled (default behavior of the hardware) b) hardware prefetching is disabled (by setting bit 0 and 1 in Model-specific Register (MSR) at address 0x1A4) as disclosed by Intel~\cite{intel-prefetching}.

\subsection{Pipeline Sequences}\label{ssec:pipeline-sequencing-algo}
A query plan is composed of several pipelines that are connected to each other. 
To execute a query, there are many possible permutations of such pipelines each with potentially different performance characteristic. 

To compare query performance with and without pipelining, we should ensure that both strategies use the same sequence.
Therefore to come up with a sequence given a query plan, we present an algorithm.
This algorithm generates a pipelining sequence that maximizes the opportunity for pipelining. 
We call this algorithm MPS (Maximal Pipeline Sequence) Algorithm.
%We now describe our algorithm for sequencing pipelines in a query plan. 

We make the following assumptions for the use of our algorithm. 
Only one pipeline gets executed at a time.
Simultaneous execution of multiple pipelines could pollute the cache, thus making it harder to reason about performance.
We assume a hash-based implementation of join algorithms.
A logical join operation has corresponding two physical operators: a build operator that builds the hash table and a probe operator that lets the input probe the hash table.
The probe operator does not begin its execution until the build operator's execution is over.
We assume no disk spilling during a query execution. 

%If (node.name() == SELECT) {
%		Return node.id();
%}
%If (node.name() == BUILD) {
%	Return GetSequence(node.inputNode()) + node.ID();
%}
%If (node.name() == PROBE) {
%		Return GetSequence(node.buildNode()) + 
%GetSequence(node.inputNode()) + node.ID();
%	}


\begin{algorithm}
	\caption{Pipeline Sequencing Algorithm}
	\begin{algorithmic}[1]
		\Function{GetSequence}{Operator op}
		\If {op.name() == {\tt select}}
		\State \Return op.ID();
		\EndIf
		\If {op.name() == {\tt build}}
		\State \Return  GetSequence(op.child()) + op.ID();
		\EndIf	
		\If {op.name() == {\tt probe}}
		\State \Return  GetSequence(op.buildOp)) + GetSequence(op.child()) + op.ID();
		\EndIf	
		\EndFunction		
			
	\end{algorithmic}
	\label{alg:pipeline-sequence}
\end{algorithm}

Algorithm~\ref{alg:pipeline-sequence} describes the MPS algorithm which is a variation of depth first traversal of the query plan DAG. 
The key heuristics behind the algorithm is to construct the hash tables for the join early and facilitate the pipelining for the probing, by delaying the execution of the probe operator.
There are two kinds of inputs for a probe operator - the probe relation and the hash table.
By nature, the accesses within the hash table are random, where as the access pattern for the probe relation is sequential. 
Therefore during the probe operation we prefer probe relation being hot in caches, as opposed to the hash table being hot.

The function \textsc{getSequence} produces a sequence of pipelines in a recursive manner.
We describe rules for each kind of operator such as \texttt{select}, \texttt{build}, and \texttt{probe}.
Each rule determines the position of the given operator w.r.t to the sequence generated by the subtree rooted at this operator.
For example, a \texttt{select} operator terminates the pipeline sequence, or a \texttt{build} operator appends itself before its subtree.

%Given a pipeline that has multiple incoming pipelines, which pipeline should be executed earlier?
%To answer this question, we sort the incoming pipelines using a custom comparator described in the function \textsc{pipelineComparator} in Line~\ref{alg:comparator}.
%The \textit{smaller} pipeline is executed earlier. 
%We now describe the sorting criterion.
%As mentioned earlier, we want the build hash operator to execute sooner, therefore we treat them \textit{smaller}, as compared to the pipelines ending with a non-build hash operator.
%In case of multiple incoming pipelines ending with build hash operator (Line~\ref{alg:break-ties}), we break the ties by declaring the pipeline with a smaller ID as smaller.
%\todo{Show an example.}